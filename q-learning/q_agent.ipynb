{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from gym.spaces import Box\n",
    "from poke_env.player import Gen9EnvSinglePlayer\n",
    "from poke_env.environment.abstract_battle import AbstractBattle\n",
    "from poke_env.player.env_player import ObservationType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPlayer(Gen9EnvSinglePlayer):\n",
    "\n",
    "    def __init__(self, battle_format=\"gen9ou\", epsilon=1.0, alpha=0.1, gamma=0.9):\n",
    "        super().__init__(battle_format=battle_format)\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.q_table = {}  # Q-table\n",
    "\n",
    "    def calc_reward(self, last_battle, current_battle) -> float:\n",
    "        return self.reward_computing_helper(\n",
    "            current_battle, fainted_value=2.0, hp_value=1.0, victory_value=30.0\n",
    "        )\n",
    "\n",
    "    # converts the battle state into a numerical vector representation \n",
    "    # extracts important battle information and transforms it into a feature vector with move power, move effectiveness, and num of fainted pokemons\n",
    "    def embed_battle(self, battle: AbstractBattle) -> ObservationType:\n",
    "        moves_base_power = -np.ones(4)\n",
    "        moves_dmg_multiplier = np.ones(4)\n",
    "        for i, move in enumerate(battle.available_moves):\n",
    "            moves_base_power[i] = move.base_power / 100 if move.base_power else -1\n",
    "            if move.type:\n",
    "                moves_dmg_multiplier[i] = move.type.damage_multiplier(\n",
    "                    battle.opponent_active_pokemon.type_1,\n",
    "                    battle.opponent_active_pokemon.type_2,\n",
    "                )\n",
    "\n",
    "        fainted_mon_team = len([mon for mon in battle.team.values() if mon.fainted]) / 6\n",
    "        fainted_mon_opponent = len([mon for mon in battle.opponent_team.values() if mon.fainted]) / 6\n",
    "\n",
    "        final_vector = np.concatenate(\n",
    "            [moves_base_power, moves_dmg_multiplier, [fainted_mon_team, fainted_mon_opponent]]\n",
    "        )\n",
    "        return np.float32(final_vector)\n",
    "\n",
    "    # defines the observation space??\n",
    "    # function ensures your AI knows what state values are valid\n",
    "    def describe_embedding(self) -> Box:\n",
    "        low = np.array([-1, -1, -1, -1, 0, 0, 0, 0, 0, 0], dtype=np.float32)\n",
    "        high = np.array([3, 3, 3, 3, 4, 4, 4, 4, 1, 1], dtype=np.float32)\n",
    "        return Box(low, high, dtype=np.float32)\n",
    "    \n",
    "    def select_action(self, battle):\n",
    "        state = tuple(self.embed_battle(battle))  # Convert state to a hashable type\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(4)\n",
    "        \n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(range(4))  # Explore\n",
    "        else:\n",
    "            return int(np.argmax(self.q_table[state]))  # Exploit\n",
    "\n",
    "    def update_q_values(self, state, action, reward, next_state):\n",
    "        state, next_state = tuple(state), tuple(next_state)  # Convert to tuple\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(4)\n",
    "\n",
    "        best_next_action = np.max(self.q_table[next_state])\n",
    "        temporal_difference = reward + (self.gamma * best_next_action) - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.alpha * temporal_difference\n",
    "\n",
    "    # def battle_end(self, battle):\n",
    "    #     self.epsilon = max(0.01, self.epsilon * 0.995)  # Gradually reduce exploration\n",
    "\n",
    "    def reset_battle(self):\n",
    "        return self.reset()  # Reset the battle environment\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.act(action)  # Execute action in battle\n",
    "\n",
    "    def train(self, num_episodes=1000):\n",
    "        for episode in range(num_episodes):\n",
    "            battle = self.reset_battle()  # Start a new battle\n",
    "            state = self.embed_battle(battle)  # Get initial state\n",
    "\n",
    "            while not battle.finished:\n",
    "                action = self.select_action(battle)\n",
    "                \n",
    "                next_battle = self.step(action)  # Perform action in battle\n",
    "                next_state = self.embed_battle(next_battle)\n",
    "                reward = self.calc_reward(battle, next_battle)\n",
    "\n",
    "                self.update_q_values(state, action, reward, next_state)\n",
    "\n",
    "                state = next_state  # Move to the next state\n",
    "\n",
    "            self.battle_end(battle)  # Reduce exploration rate after each battle\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
